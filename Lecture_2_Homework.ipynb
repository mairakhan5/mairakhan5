{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mairakhan5/mairakhan5/blob/main/Lecture_2_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data download\n",
        "#@markdown This cell contains code to retrieve the dataset for our tutorial. You can ignore its contents, for the most part\n",
        "! pip install gdown > /dev/null\n",
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1GxY6eXFRgLNTbfCouiIsKC4GZQWoXwAE'\n",
        "gdown.download(url, 'titanic_data.csv', quiet=False)\n",
        "url = 'https://drive.google.com/uc?id=1uWTuZ2p0iVwvzssLON2MHb1sXCXeC_qC'\n",
        "gdown.download(url, 'titanic_test.csv', quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "lf9hd_VGK9wL",
        "outputId": "4a39d1ab-ada3-4f50-9afc-2dd6dddcbbee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GxY6eXFRgLNTbfCouiIsKC4GZQWoXwAE\n",
            "To: /content/titanic_data.csv\n",
            "100%|██████████| 157k/157k [00:00<00:00, 72.2MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'titanic_data.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework for session 2\n",
        "\n",
        "## Revisiting the Titanic dataset\n",
        "\n",
        "In this assignment we will be revisiting the Titanic dataset that we used in session 1. As a reminder, the Titanic dataset is a well-known dataset that contains information about the passengers who were on board the RMS Titanic when it sank on April 15, 1912. The dataset includes information about the passengers' demographics, ticket information, and survival status. It is a popular dataset for machine learning and data analysis because of its size and the historical significance of the event.\n",
        "\n",
        "The features of the Titanic dataset are:\n",
        "\n",
        "- `Survived`: the target variable, whether the passenger survived or not (0 = No, 1 = Yes)\n",
        "- `Pclass`: the passenger class (1 = First class, 2 = Second class, 3 = Third class)\n",
        "- `Name`: the name of the passenger\n",
        "- `Sex`: the gender of the passenger\n",
        "- `Age`: the age of the passenger\n",
        "- `SibSp`: the number of siblings/spouses aboard the Titanic\n",
        "- `Parch`: the number of parents/children aboard the Titanic\n",
        "- `Ticket`: the ticket number\n",
        "- `Fare`: the fare paid for the ticket\n",
        "- `Cabin`: the cabin number\n",
        "- `Embarked`: the port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n"
      ],
      "metadata": {
        "id": "9UClUwisKBgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a logistic regression\n",
        "\n",
        "Read the dataset (`titanic_data.csv`) using Pandas, and run a quick Exploratory Data Analysis to remind you of the relationships between the variables. Then, transform the DataFrame into an array of features (your independent variables, or $x$) and an array of labels (your dependent variable, or $y$) and train a logistic regression model as we did in the lecture.\n",
        "\n",
        "**Note**: sci-kit learn functions only accept 2D arrays with the shape ``(n_examples, n_features)``. To transform a Pandas DataFrame into the correct shape, you can use the following idiom:\n",
        "\n",
        "```python\n",
        "data_x = df[['feature_1', 'feature_2']].to_numpy()\n",
        "data_y = df[['target_variable']].to_numpy().reshape(-1, 1)\n",
        "```"
      ],
      "metadata": {
        "id": "yHAxfY1-Kycd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Transform your DataFrame into arrays for sci-kit learn\n",
        "df = pd.read_csv('titanic_data.csv')\n",
        "data_x = # ...\n",
        "data_y = # ...\n",
        "\n",
        "# Train / test / validation split\n",
        "train_x, test_x, train_y, test_y = train_test_split(\n",
        "    # ...\n",
        ")\n",
        "train_x, valid_x, train_y, test_y = train_test_split(\n",
        "    # ...\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression()\n",
        "model.fit(train_x, train_y)\n",
        "# ..."
      ],
      "metadata": {
        "id": "C7-9WA7YKxIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimising hyperparameters\n",
        "\n",
        "In the lecture we discussed the concept of *overfitting*, which is when the model starts learning the intrinsic noise in the data, rather than the underlying patterns. While we considered techniques such as cross-validation and train/test split to detect overfitting, we did not discuss the many techniques used to actually *reduce* overfitting, which are generally known under the name of **regularisation**.\n",
        "\n",
        "Sci-kit learn implementations often come with some sort of regularisation to improve results and stabilise training. However, this comes at a price: regularisation forces us to choose some additional parameters (generally referred to as **hyperparameters**) that can have a dramatic effect on the performance of our models. Good data science practice often requires to test a variety of hyperparameters before deciding on the optimal choices for the problem at hand.\n",
        "\n",
        "In this assignment, I suggest that you try to optimise the hyperparameters for the Logistic Regression model. You should split your training data into training and validation, and see how different choices affect the performance on the held-out validation set. You may also want to check the [LogisticRegression *sci-kit learn* documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) for the hyperparameters."
      ],
      "metadata": {
        "id": "Ad8BE5w6Lb04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ..."
      ],
      "metadata": {
        "id": "mzuixYN4LbPW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}